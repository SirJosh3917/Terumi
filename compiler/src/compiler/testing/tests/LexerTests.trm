use std
use compiler
use compiler.lexer
use compiler.testing

class LexerTests
{
	string test_group
	ContractTestHelper runner

	ctor() {
		test_group = "Lexer"
	}
	
	begin(string name) runner.begin(name)
	assert(bool condition) runner.assert(condition)
	
	test(ContractTestHelper test_runner) {
		runner = test_runner
		number IDENTIFIER_TOKEN = 1
		number SPECIAL_TOKEN = 2
		number NUMBER_TOKEN = 3
		number EOF_TOKEN = 4
		number STRING_TOKEN = 6

		begin("next_token() yields identifier")
		Lexer lexer = new Lexer("asdfgh")
		Token token = lexer.next_token()
		
		assert(token.token_type == IDENTIFIER_TOKEN)
		assert(token.source.equals("asdfgh"))
		
		// add whitespace checking
		lexer = new Lexer("		 	     	  hello")
		token = lexer.next_token()
		assert(token.token_type == IDENTIFIER_TOKEN)
		assert(token.source.equals("hello"))
		
		begin("next_token() yields number")
		lexer = new Lexer("12345")
		token = lexer.next_token()
		assert(token.token_type == NUMBER_TOKEN)
		assert(token.source.equals("12345"))
		
		// with whitespace tests
		lexer = new Lexer("  			 	 	 	  	420")
		token = lexer.next_token()
		assert(token.token_type == NUMBER_TOKEN)
		assert(token.source.equals("420"))
		
		begin("next_token() yields special")
		lexer = new Lexer(".")
		token = lexer.next_token()
		assert(token.token_type == SPECIAL_TOKEN)
		assert(token.source.equals("."))
		
		// with whitespace
		lexer = new Lexer(" 	 	 	  	 			   ;")
		token = lexer.next_token()
		assert(token.token_type == SPECIAL_TOKEN)
		assert(token.source.equals(";"))
		
		begin("next_token() yields eof")
		lexer = new Lexer("")
		token = lexer.next_token()
		assert(token.token_type == EOF_TOKEN)
		assert(token.source.equals(""))
		
		// should continue to yield EOF
		token = lexer.next_token()
		assert(token.token_type == EOF_TOKEN)
		assert(token.source.equals(""))
		
		// whitespace
		lexer = new Lexer(" 		  		 	 	 	 	 	 ")
		token = lexer.next_token()
		assert(token.token_type == EOF_TOKEN)
		assert(token.source.equals(""))
		
		begin("next_token() yields string")
		lexer = new Lexer("{get_quotation_mark()}a really cool string{get_quotation_mark()}")
		token = lexer.next_token()
		assert(token.token_type == STRING_TOKEN)
		assert(token.source.equals("{get_quotation_mark()}a really cool string{get_quotation_mark()}"))
		
		// whitespace
		lexer = new Lexer(" 		  		 	 	 	 	 	 {get_quotation_mark()}a cool string{get_quotation_mark()}hfdshheh3h34hd")
		token = lexer.next_token()
		assert(token.token_type == STRING_TOKEN)
		assert(token.source.equals("{get_quotation_mark()}a cool string{get_quotation_mark()}"))
		
		lexer = new Lexer("{get_quotation_mark()}escape {get_backslash()}{get_quotation_mark()}{get_quotation_mark()}dsfgfdsgfsd")
		token = lexer.next_token()
		assert(token.token_type == STRING_TOKEN)
		assert(token.source.equals("{get_quotation_mark()}escape {get_backslash()}{get_quotation_mark()}{get_quotation_mark()}"))
		
		lexer = new Lexer("{get_quotation_mark()}escape {get_backslash()}{get_backslash()}{get_backslash()}{get_quotation_mark()}{get_quotation_mark()}asefasdf")
		token = lexer.next_token()
		assert(token.token_type == STRING_TOKEN)
		assert(token.source.equals("{get_quotation_mark()}escape {get_backslash()}{get_backslash()}{get_backslash()}{get_quotation_mark()}{get_quotation_mark()}"))
		
		begin("next_token() skips comments")
		lexer = new Lexer("// singleline comment

")
		token = lexer.next_token()
		assert(token.is_newline())
		lexer = new Lexer("/*
	multline
	comment
	*/asdf
")
		token = lexer.next_token()
		assert(token.is_identifier())
		
		lexer = new Lexer("        /* whitespace
	and comments */                // intermixed
")
		token = lexer.next_token()
		assert(token.is_newline())
		
		begin("scan_to() rescans token")
		lexer = new Lexer("asdf ghjk")
		token = lexer.next_token()
		lexer.scan_to(token)
		Token other_token = lexer.next_token()
		assert(token.token_type == other_token.token_type)
		assert(token.source.equals_text(other_token.source))
		
		begin("scan_after() scans next token")
		lexer = new Lexer("asdf ghjk")
		token = lexer.next_token()
		Token temp = lexer.next_token()
		lexer.scan_after(token)
		other_token = lexer.next_token()
		assert(token.source.equals("asdf"))
		assert(other_token.source.equals("ghjk"))
		
		begin("next_token() for interpolated string")
		
		// expressions should be parsed inside of interpolations, no matter if they're invalid or not
		lexer = new Lexer("{get_quotation_mark()}some \{1234\} text{get_quotation_mark()}")
		token = lexer.next_token()
		assert(token.token_type == STRING_TOKEN)
		assert(token.source.equals("{get_quotation_mark()}some \{1234\} text{get_quotation_mark()}"))
		
		// a string is an expression, should be handled
		lexer = new Lexer("{get_quotation_mark()}some \{{get_quotation_mark()}inner string{get_quotation_mark()}\} text{get_quotation_mark()}")
		token = lexer.next_token()
		assert(token.token_type == STRING_TOKEN)
		assert(token.source.equals("{get_quotation_mark()}some \{{get_quotation_mark()}inner string{get_quotation_mark()}\} text{get_quotation_mark()}"))
		
		// a string is an expression, should be handled (with inner strings too)
		// TODO: make test pass in far future
		lexer = new Lexer("{get_quotation_mark()}some \{{get_quotation_mark()}inner \{{get_quotation_mark()}inner string{get_quotation_mark()}\} string{get_quotation_mark()}\} text{get_quotation_mark()}")
		token = lexer.next_token()
		assert(token.token_type == STRING_TOKEN)
		assert(token.source.equals("{get_quotation_mark()}some \{{get_quotation_mark()}inner \{{get_quotation_mark()}inner string{get_quotation_mark()}\} string{get_quotation_mark()}\} text{get_quotation_mark()}"))
	}
}